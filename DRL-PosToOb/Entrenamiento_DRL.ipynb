{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623bf6cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizacion iniciada!\n",
      "WARNING:tensorflow:From C:\\Users\\roger\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Optimizacion terminada!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from EnvCoppSim import Robot\n",
    "from detection import ObjectDetector\n",
    "import network_utils\n",
    "\n",
    "# Variables\n",
    "\n",
    "steps = 10000 # cambiar a 5000\n",
    "start_step = 0\n",
    "episode = 0\n",
    "display_step = 10\n",
    "\n",
    "batch_size = 70\n",
    "memory_size = batch_size * 50\n",
    "\n",
    "noise_theta = 0.02\n",
    "noise_sigma = 0.025\n",
    "\n",
    "weight_update_rate = 0.001\n",
    "\n",
    "\n",
    "# Calculo de la recompensa por una accion\n",
    "\n",
    "def limit_over(x, min, max):\n",
    "    \"\"\"\n",
    "        Valor absoluto, que valor esta fuera de rango\n",
    "\n",
    "        Parametros\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            Valores investigados\n",
    "        min : ndarray\n",
    "            Valores minimo de rango \n",
    "        max : ndarray\n",
    "            Valores maximos de rango\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        ndarray\n",
    "            Dimensiones fuera de rango\n",
    "    \"\"\"\n",
    "    return np.maximum(min - x, 0) + np.maximum(x - max, 0)\n",
    "\n",
    "def estimate_state_value(state, pos_t, joint_ranges):\n",
    "    \"\"\"\n",
    "        Evaluar condicion\n",
    "\n",
    "        Parametros\n",
    "        ----------\n",
    "        state : ndarray\n",
    "            Vector de estado\n",
    "        pos_t : ndarray\n",
    "            Coordenadas generalizadas de destino\n",
    "        joint_ranges : ndarray\n",
    "            Rangos de coordenadas\n",
    "        Retorno\n",
    "        -------\n",
    "        float\n",
    "            valor del estado\n",
    "        int\n",
    "            1 - estado finaliza el episodio, 0 - no\n",
    "        dict\n",
    "            Metrica\n",
    "    \"\"\"\n",
    "\n",
    "    obj_area = state[3]*state[4]\n",
    "    offset = np.sqrt(state[1]**2 + state[2]**2)\n",
    "    depth = state[0]\n",
    "\n",
    "    # La salida del movimiento establecido fuera de los limites de una zona de trabajo es facil de rechazar. \n",
    "    # Sin embargo, se deben deducir puntos por ello para dar a la red una retroalimentacion mas rapida sobre la idoneidad de las acciones.\n",
    "    limit_penalty = 0.25 * np.mean(limit_over(pos_t, joint_ranges[:,0], joint_ranges[:,1]))\n",
    "\n",
    "    value = obj_area + 0.5*(0.71 - offset) + 0.25*(1-depth) - limit_penalty\n",
    "    terminal = 0\n",
    "    if obj_area < 0.001:\n",
    "        value += -1\n",
    "        terminal = 1\n",
    "    elif value > 0.2 + 0.3 + 0.2:\n",
    "        value += 1\n",
    "        terminal = 1\n",
    "    return value, terminal, {\"limit_penalty\": limit_penalty}\n",
    "\n",
    "def compute_reward(state_value, next_state_value, terminal, episode_time, action):\n",
    "    \"\"\"\n",
    "        Calcular recompensa\n",
    "\n",
    "        Parametros\n",
    "        ----------\n",
    "        state_value : float\n",
    "            valor del estado\n",
    "        next_state_value : float\n",
    "            Valor de estado siguiente\n",
    "        terminal : int\n",
    "            1 - estado finaliza el episodio, 0 - no\n",
    "        episode_time : int\n",
    "            Duracion del episodio\n",
    "        action : ndarray\n",
    "            Accion - cambios por coordenadas generalizadas\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        float\n",
    "            Recompensa por accion\n",
    "        int\n",
    "            1 - estado finaliza el episodio, 0 - no\n",
    "        list\n",
    "            Metrica\n",
    "    \"\"\"\n",
    "\n",
    "    if episode_time > 15:\n",
    "        return -0.5, 1, [0,0]\n",
    "    else:\n",
    "        rates = np.asarray([2,1,2,1,2,3])\n",
    "        L1 = np.sum(abs(action)*rates)\n",
    "        # El tamano de la jugada puede hacer que el objeto se pierda, provocando asi una penalizacion.\n",
    "        size_penalty = 0.5 * np.maximum(L1 - 1.4, 0)\n",
    "        state_value_reward = next_state_value - state_value\n",
    "        reward = state_value_reward + 0.03 - size_penalty\n",
    "        \n",
    "        return reward, terminal, { \"size_penalty\": size_penalty, \"state_value_reward\": state_value_reward }\n",
    "\n",
    "\n",
    "# Fabricas para aprender funciones de paso\n",
    "\n",
    "def create_actor_train_step(actor, critic, actor_optimizer, summary_writer):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def actor_train_step(states, step):\n",
    "        with summary_writer.as_default():\n",
    "            actions = actor(states, training=True)\n",
    "            q = critic([actions, states], training=False)\n",
    "            if step % display_step == 0:\n",
    "                tf.summary.scalar('actor_q', tf.reduce_mean(q[:-5]), step)\n",
    "            \n",
    "            # Calculo de derivadas con respecto a los pesos de la red-actor,\n",
    "            # usando derivadas parciales de Q con respecto a la acción multidimensional (cambios en coordenadas generalizadas)\n",
    "            action_gradients = tf.concat(tf.gradients(q, actions), axis=0)\n",
    "            vars = actor.trainable_variables\n",
    "            unnormalized_gradients = tf.gradients(actions, vars, -action_gradients)\n",
    "            count = tf.cast(tf.shape(actions)[0], dtype=tf.float32)\n",
    "            normalized_gradients = list(map(lambda x: tf.math.divide(x, count), unnormalized_gradients))\n",
    "            actor_optimizer.apply_gradients(zip(normalized_gradients, vars))\n",
    "    return actor_train_step\n",
    "\n",
    "def create_critic_train_step(actor, actor_target, critic, critic_target, critic_optimizer, summary_writer):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def critic_train_step(states, actions, rewards, next_states, terminals, step):\n",
    "        terminals = tf.cast(terminals, dtype=tf.float32)\n",
    "        terminals = tf.expand_dims(terminals, axis=1)\n",
    "        rewards = tf.cast(rewards, dtype=tf.float32)\n",
    "        rewards = tf.expand_dims(rewards, axis=1)\n",
    "        with summary_writer.as_default():\n",
    "            with tf.GradientTape() as tape:\n",
    "                # La siguiente accion se calcula utilizando una copia del actor cuyos pesos se suavizan exponencialmente\n",
    "                next_actions = actor_target(next_states, training=False)\n",
    "                next_actions = tf.stop_gradient(next_actions)\n",
    "                # La puntuación Q de la siguiente acción también se calcula utilizando la copia del crítico.\n",
    "                q_t = critic_target([next_actions, next_states], training=False)\n",
    "                q_t *= 1 - terminals\n",
    "                q_t = tf.stop_gradient(q_t)\n",
    "                q = critic([actions, states], training=True)\n",
    "                labels = rewards + 0.99 * q_t\n",
    "                cost = tf.reduce_mean(tf.keras.losses.MSE(labels, q))\n",
    "                \n",
    "                if step % display_step == 0:\n",
    "                    tf.summary.scalar('critic_cost', cost, step)\n",
    "\n",
    "                vars = critic.trainable_variables\n",
    "                gradients = tape.gradient(cost, vars)\n",
    "                critic_optimizer.apply_gradients(zip(gradients, vars))\n",
    "    return critic_train_step\n",
    "\n",
    "\n",
    "# Entidades adicionales para mejorar el aprendizaje\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "        Buffer para reproducir la experiencia previa\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def append(self, entry):\n",
    "        self.memory.append(entry)\n",
    "\n",
    "    def sample(self, n):\n",
    "        if n > len(self.memory):\n",
    "            n = len(self.memory)\n",
    "        entries = random.sample(self.memory, n)\n",
    "        batch = []\n",
    "        entry_len = len(entries[0])\n",
    "        for i in range(entry_len):\n",
    "            temp = []\n",
    "            for j in range(n):\n",
    "                temp.append(entries[j][i])\n",
    "            batch.append(np.stack(temp, axis=0))\n",
    "        return batch\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"\n",
    "        Ruido de Ornstein-Uhlenbeck\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu, theta, sigma, shape):\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.shape = shape\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x = np.ones(self.shape, dtype=np.float32) * self.mu\n",
    "        \n",
    "    def sample(self):\n",
    "        dx = self.theta * (self.mu - self.x) + self.sigma * np.random.normal(size=self.shape)\n",
    "        self.x += dx\n",
    "        return self.x\n",
    "\n",
    "\n",
    "# Inicializacion de objetos\n",
    "\n",
    "robot = Robot()\n",
    "robot.enable_synchronization() # el entrenamiento se lleva a cabo en un escenario estatico en modo de sincronizacion\n",
    "detector = ObjectDetector()\n",
    "\n",
    "actor = network_utils.create_control_network()\n",
    "actor_target = network_utils.create_control_network()\n",
    "network_utils.update_model(actor_target, actor, 1)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "critic = network_utils.create_qnetwork()\n",
    "critic_target = network_utils.create_qnetwork()\n",
    "network_utils.update_model(critic_target, critic, 1)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\"logs\")\n",
    "actor_train_step = create_actor_train_step(actor, critic, actor_optimizer, summary_writer)\n",
    "critic_train_step = create_critic_train_step(actor, actor_target, critic, critic_target, critic_optimizer, summary_writer)\n",
    "\n",
    "memory = ReplayMemory(memory_size)\n",
    "noise = OUNoise(0, noise_theta, noise_sigma, (6))\n",
    "\n",
    "\n",
    "# Entrenamiento\n",
    "\n",
    "print(\"Optimizacion iniciada!\")\n",
    "terminal = 1\n",
    "total_reward = 0\n",
    "episode_time = 0\n",
    "for step in tf.range(start_step, start_step + steps + 1, dtype=tf.int64):\n",
    "    # Inicializando un nuevo intento\n",
    "    if terminal == 1:\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('total_reward', total_reward, episode)\n",
    "            tf.summary.scalar('episode_time', episode_time, episode)\n",
    "        episode += 1\n",
    "        episode_time = 0\n",
    "        total_reward = 0\n",
    "        noise.reset()\n",
    "        while terminal == 1:\n",
    "            # Despues del reinicio, el objetivo debe estar en la linea de vision del objeto., \n",
    "            # este bucle protege contra posibles excepciones\n",
    "            robot.reset(is_dynamic=False, do_orientate=True)\n",
    "            next_state = network_utils.get_state(robot, detector)\n",
    "            pos = network_utils.extract_pos(next_state)\n",
    "            next_state_value, terminal, _ = estimate_state_value(next_state, pos, robot.joint_ranges)\n",
    "    \n",
    "    state, state_value = next_state, next_state_value\n",
    "    episode_time += 1\n",
    "\n",
    "    # Las acciones en los primeros pasos estan determinadas por ruido aleatorio para concentrarse en la expansion\n",
    "    # experimentar y fomentar el uso de pequeños movimientos, ya que el objetivo está inicialmente en la linea de vision\n",
    "    if step > 100:\n",
    "        action = actor(np.expand_dims(state, axis=0), training=False).numpy()\n",
    "        action = np.squeeze(action, axis=0)\n",
    "    else:\n",
    "        action = np.zeros((6), dtype=np.float32)\n",
    "    action += noise.sample()\n",
    "    pos = network_utils.extract_pos(state)\n",
    "    robot.set_position(pos + action)\n",
    "\n",
    "    # Determinar los estados y recompensas que siguen a la accion, almacenar el conjunto en la memoria\n",
    "    next_state = network_utils.get_state(robot, detector)\n",
    "    next_state_value, terminal, state_metrics = estimate_state_value(next_state, pos + action, robot.joint_ranges)\n",
    "    reward, terminal, reward_metrics = compute_reward(state_value, next_state_value, terminal, episode_time, action)\n",
    "    total_reward += reward\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('reward', reward, step)\n",
    "        for key in state_metrics:\n",
    "            tf.summary.scalar(key, state_metrics[key], step)\n",
    "        for key in reward_metrics:\n",
    "            tf.summary.scalar(key, reward_metrics[key], step)\n",
    "    memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "    # Actualización de pesos en base a la experiencia\n",
    "    states, actions, rewards, next_states, terminals = memory.sample(batch_size)\n",
    "    critic_train_step(states, actions, rewards, next_states, terminals, step)\n",
    "    actor_train_step(states, step)\n",
    "    network_utils.update_model(actor_target, actor, weight_update_rate)\n",
    "    network_utils.update_model(critic_target, critic, weight_update_rate)\n",
    "\n",
    "    summary_writer.flush()\n",
    "    int_step = int(step)\n",
    "    if (int_step % 1000) == 0:\n",
    "        actor.save_weights(\"actor_red\\\\checkpoint-\" + str(int_step))\n",
    "        critic.save_weights(\"critico_red\\\\checkpoint-\" + str(int_step))\n",
    "actor.save(\"modelo1.h5\")\n",
    "print(\"Optimizacion terminada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000b220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
